{"metadata":{"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import yfinance as yf\nimport csv\nimport pandas as pd","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load and Save the Stock Price Data","metadata":{}},{"cell_type":"code","source":"tsla = yf.download(\"TSLA\", start='2020-01-01',end='2024-06-28')\ntsla.to_csv(\"data/tsla_20200101_20240628.csv\")\ntsla","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsla.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft = yf.download(\"MSFT\", start='2020-01-01',end='2024-06-28')\nmsft.to_csv(\"data/msft_20200101_20240628.csv\")\nmsft.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nfrom matplotlib.pylab import rcParams\nimport seaborn as sns\nfrom numpy.random import seed\nseed(0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"### Load Data","metadata":{}},{"cell_type":"code","source":"# load stock data\nmsft = pd.read_csv('./data/msft_20200101_20240628.csv')\ntsla = pd.read_csv('./data/tsla_20200101_20240628.csv')\n# load sentiment data\nmsft_sent = pd.read_csv('./data/msft_with_sentiment_label.csv')\ntsla_sent = pd.read_csv('./data/tsla_with_sentiment_label.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"company_list = [msft,tsla]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA of Microsoft Data","metadata":{}},{"cell_type":"code","source":"print(\"###########Microsoft Stock Dataframe Info#############\")\nmsft.info()\nprint(\"###########Microsoft Sentiment Analysis Dataframe Info#############\")\nmsft_sent.info()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_style('whitegrid')\nplt.style.use('fivethirtyeight')\n%matplotlib inline","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"end_day = datetime(2024,6,28)\nstart_day = datetime(2020,1,1)\n\nnum_of_days = end_day - start_day\nnum_of_days = num_of_days.days\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft['Date'] = pd.to_datetime(msft['Date'])\nplt.figure(figsize=(15,10))\nplt.plot(msft['Date'],msft['Adj Close'])\n# msft.plot()\nplt.ylabel('Adj Close')\nplt.xlabel(None)\nplt.xticks(rotation = 45 )\nplt.title(\"Line Chart of Microsoft Stock Price\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft['Date'] = pd.to_datetime(msft['Date'])\nmsft_sent['date'] = pd.to_datetime(msft_sent['date'])\ntsla['Date'] = pd.to_datetime(tsla['Date'])\ntsla_sent['date'] = pd.to_datetime(tsla_sent['date'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### What was the moving average of the stocks?\n- The moving average (MA) is a simple technical analysis tool that smooths out price data by creating a constantly updated average price. The average is taken over a specific period of time, like 10 days, 20 minutes, 30 weeks, or any time period the trader chooses.","metadata":{}},{"cell_type":"code","source":"ma_day = [10,20,50]\n\nfor ma in ma_day:\n    for company in company_list:\n        column_name = f\"MA for {ma} days\"\n        company[column_name] = company[\"Adj Close\"].rolling(ma).mean()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, axes = plt.subplots(nrows =2, ncols = 2)\n# fig.set_figheight(10)\n# fig.set_figwidth(15)\n\n# msft[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,0])\n# axes[0,0].set_title('Microsoft')\n# plt.figure(figsize=(15,10))\nmsft[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(figsize=(15,10),title='Microsoft Stock Price every 10,20,50 rolling days')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### retrieve missing dates\npd.date_range(start = '2020-01-01', end = '2024-06-28' ).difference(msft.Date)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA of Tesla Data","metadata":{}},{"cell_type":"code","source":"print(\"###########Tesla Stock Dataframe Info#################\")\ntsla.info()\nprint(\"###########Tesla Sentiment Analysis Dataframe Info#################\")\ntsla_sent.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsla.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsla['Date'] = pd.to_datetime(tsla['Date'])\nplt.figure(figsize=(15,10))\nplt.plot(tsla['Date'],tsla['Adj Close'])\n# msft.plot()\nplt.ylabel('Adj Close')\nplt.xlabel(None)\nplt.xticks(rotation = 45 )\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsla['Date'] = pd.to_datetime(tsla['Date'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tsla[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(figsize=(15,10),title='Tesla Stock Price every 10,20,50 rolling days')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handle missing dates","metadata":{}},{"cell_type":"code","source":"msft_sent.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft_sent['label'].value_counts(dropna=False).to_frame('count').join(\n    msft_sent['label'].value_counts(dropna=False, normalize=True).to_frame('normalize')\n).round(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"51.8567 % 마이크로 소프트 감정라벨이 Neutral임\n","metadata":{}},{"cell_type":"code","source":"# 1. 우선, sentiment에서 주식시장 안열린 날짜의 뉴스는 제외한다\nmissing_stock_dates = pd.date_range(start = '2020-01-01', end = '2024-06-28' ).difference(msft.Date) # 주식거래 없는 날\nmissing_sent_dates = pd.date_range(start = '2020-01-01', end = '2024-06-28' ).difference(msft_sent.date) # 뉴스 없는 날\n#-> 이 날짜를 제외한 모든 df의 rows 들을 가져온다.\nno_trade_dates = missing_stock_dates.tolist()\nmsft_sent.loc[(msft_sent[\"date\"].isin(no_trade_dates))]\n# msft_sent[~msft.index.isin(no_trade_dates)]\n# 2. 주식거래 안하는 날짜를 제외한 후, 뉴스 기사가 없는날짜가 몇개인지 확인한다.\n# 3. 각 날짜별 최대 뉴스 갯수가 몇개인지 확인.\n# 4. 각 감정 라벨별의 분포를 확인한다. ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1535 개의 articles 중 147개의 article은 trade가 없는 날의 기사임.","metadata":{}},{"cell_type":"code","source":"# trade가 있는 날의 msft 기사\nmsft_sent_trade = msft_sent.loc[~(msft_sent[\"date\"].isin(no_trade_dates))].reset_index()\nmsft_sent_trade","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_date_list = msft['Date'].tolist()\ntrading_day_num = len(stock_date_list)\nprint(f'Total number of stock trading days :{len(stock_date_list)}')\nsent_date_list = msft_sent_trade['date'].tolist()\n\nfor date in sent_date_list:\n    if date in stock_date_list:\n        stock_date_list.remove(date)\nprint(f'Total number of no article days during stock trading days :{len(stock_date_list)}')\n\nprint(f'Percentage of missing article days : {(len(stock_date_list)/trading_day_num)*100} %')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stock_date_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Percentages and count of each sentiment label","metadata":{}},{"cell_type":"code","source":"# 개수랑 비율 같이. column명도 바꿔서 표시\nsentiment_count = msft_sent_trade['label'].value_counts(dropna=False).to_frame('count').join(\n    msft_sent_trade['label'].value_counts(dropna=False, normalize=True).to_frame('normalize')\n).round(4)\nsentiment_count","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Defining colors for the pie chart \ncolors = ['pink', 'silver', 'steelblue'] \n  \n# Define the ratio of gap of each fragment in a tuple \nexplode = (0.05, 0.05, 0.05) \nsentiment_count.plot.pie(y='count',figsize=(5, 5),subplots=True,autopct='%1.0f%%', \n  colors=colors, explode=explode)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"article_count_by_date = msft_sent_trade['date'].value_counts(dropna=False).to_frame('count')\narticle_count_by_date['date'] = article_count_by_date.index\narticle_count_by_date\n# article_count_by_date.reset_index()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft_sent_trade[msft_sent_trade['date'] == '2023-11-20']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize=(20,10))\nax.bar(article_count_by_date['date'], article_count_by_date['count'], width=3,color='#008080')\nax.xaxis_date()\nplt.title('The number of articles during trading days')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Score","metadata":{}},{"cell_type":"markdown","source":"### Data Preprocessing","metadata":{}},{"cell_type":"code","source":"msft_sent = pd.read_csv('./data/msft_with_sentiment_label.csv')\nmsft_sent","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft_sent = msft_sent.drop(['input'],axis = 1)\nmsft_sent = msft_sent.sort_values(by='date')\nmsft_sent","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft_sent.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add the numeric label of sentiment\n- if label is Positive, 1\n- if label is Neutral, 0\n- if label is Negative, -1","metadata":{}},{"cell_type":"code","source":"\nmsft_sent['sentiment'] = msft_sent[\"label\"].apply(lambda x: 1 if x == 'positive' else (-1 if x == 'negative' else 0))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft_sent","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_score = msft_sent.groupby([msft_sent['date']])['sentiment'].mean().to_frame('score')\nsent_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_score.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'@@@@@@@@@@@@Negative Sentiment@@@@@@@@@@@@@@@')\nneg = sent_score[sent_score['score'] <0].count().tolist()[0]\nprint(f'Count : {neg}')\nprint(f'Percentage: {(neg/len(sent_score))*100:.2f} %')\nprint(f'@@@@@@@@@@@@Neutral Sentiment@@@@@@@@@@@@@@@')\nneu = sent_score[sent_score['score'] ==0].count().tolist()[0]\nprint(f'Count : {neu}')\nprint(f'Percentage: {(neu/len(sent_score))*100:.2f} %')\n\nprint(f'@@@@@@@@@@@@Positive Sentiment@@@@@@@@@@@@@@@')\npos = sent_score[sent_score['score'] >0].count().tolist()[0]\nprint(f'Count : {pos}')\nprint(f'Percentage: {(pos/len(sent_score))*100:.2f} %')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(msft_sent['date'].unique().tolist())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_score['score'].unique()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_score['date'] = sent_score.index\nsent_score = sent_score.reset_index(drop=True)\nsent_score\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_score['date'] = pd.to_datetime(sent_score['date'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sent_score.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"score_date_list =sent_score['date'].dt.strftime('%Y-%m-%d').tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"whole_dates = pd.date_range(start = '2020-01-02', end = '2024-06-28' ).strftime('%Y-%m-%d').tolist()\n# missing_sent_dates = pd.date_range(start = '2020-01-01', end = '2024-06-28' ).difference(msft_sent.date) # 뉴스 없는 날\nno_sent_dates = list(set(whole_dates) - set(score_date_list)) \n# no_sent_dates\nno_sent_dates.sort(reverse=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import timedelta\nimport math \n\nadded_sent_score = sent_score\ntmp = []\n\nfor d in no_sent_dates:\n    n = 1\n    while True:\n        day_before = (datetime.strptime(d, '%Y-%m-%d')-timedelta(days=n)).strftime('%Y-%m-%d')\n        print(day_before)\n        n_day_before_sent = sent_score[sent_score['date'].dt.strftime('%Y-%m-%d') == day_before]['score'].astype('float').tolist()\n        if len(n_day_before_sent) == 0:\n            n+=1\n            continue\n        else:\n            print(f'{d} : {n_day_before_sent} * 0.5 to the power of {n}')\n            score = n_day_before_sent[0] * math.pow(0.5,n)\n            new_row = {'date': d, 'score': score}\n            tmp.append(new_row)\n            print(f'{d} :{score}')\n            break\n    \ntmp","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"no_article_day_sent_score = pd.DataFrame(tmp,columns=['date','score'])\nno_article_day_sent_score","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a = sent_score[sent_score['date'].dt.strftime('%Y-%m-%d') == '2024-06-34']['score'].astype('float').tolist()\na","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datetime import timedelta\n# date'2022-05-07'\n(datetime.strptime('2022-05-07', '%Y-%m-%d')-timedelta(days=1)).strftime('%Y-%m-%d')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleansing and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"# Feature Scaling","metadata":{}},{"cell_type":"code","source":"msft.drop(\"Date\",axis=1,inplace=True)\nmsft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nmsft.iloc[:,:] = scaler.fit_transform(msft.iloc[:,:])\nmsft","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"msft.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train & Test set Split","metadata":{}},{"cell_type":"code","source":"\nimport random\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pandas.tseries.offsets import BDay\n\n# Visualization Imports\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.io as pio\nimport scipy.stats as stats\n\n# Neural Network Imports\nimport tensorflow as tf\nfrom tensorflow.keras import models\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Setting seed\nSEED = 0\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Visualization Configurations\npio.templates.default = \"plotly_dark\"\n%config InlineBackend.figure_format = 'retina'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns = ['open', 'high', 'low', 'close', 'volume']\n\nmsft.columns = msft.columns.str.lower()\n\n# Showing data\nmsft.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Data Prep","metadata":{}},{"cell_type":"markdown","source":"<center>\n<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\"  width=\"60%\" height=\"30%\">\n</center>\n\nIn machine learning, it's often crucial, especialy when using neural networks, to normalize data before feeding it into a model. This process adjusts values measured on different scales to a notionally common scale, often prior to averaging. Here, we use the `MinMaxScaler` from the `sklearn.preprocessing` package, which scales each feature by its maximum and minimum values. This scaler transforms each value `v` in a feature column to `v'` in the range [0, 1] using the following formula:\n$$v' = \\frac{v - \\text{min}(v)}{\\text{max}(v) - \\text{min}(v)}$$\n- `v` is the original value.\n- `min(v)` is the minimum value in the feature column.\n- `max(v)` is the maximum value in the feature column.\nThe columns `['open', 'high', 'low', 'close', 'volume']` from the Tesla stock dataset are normalized, which includes the opening, high, low, and closing prices along with the trading volume. Normalizing these features allows for a more stable and faster convergence during the training of neural networks, like the LSTM model we'll be using for stock price forecasting.","metadata":{}},{"cell_type":"code","source":"class RNNFormater:\n    \n    def __init__(self, data: pd.DataFrame, mapping_steps=10):\n        \"\"\"\n        Initialize the RNNFormater with a DataFrame and steps to map for data.\n        \n        Args:\n            data (pd.DataFrame): Input DataFrame containing time series data.\n            mapping_steps (int): Number of time steps for each input sequence to be mapped to output.\n        \"\"\"\n        # Storing data\n        self.df = data.copy()\n        self.data = self.df.values\n\n        # Scaler stored for usage later\n        self.scaler = MinMaxScaler()\n        self.normalized_data = self.scaler.fit_transform(self.data)\n        \n        self.time_steps = data.shape[0]\n        self.n_columns = data.shape[1]\n\n        # Number of mapping steps\n        self.mapping_steps = mapping_steps\n\n    def data_mapping(self):\n        \"\"\"\n        Maps a 2D array into a 3D array for RNNs input, with each sequence having mapping_steps time steps.\n    \n        Args:\n            mapping_steps (int): Number of time steps for each sequence.\n    \n        Returns:\n            np.array: A 3D array suitable for RNN inputs.\n        \"\"\"\n        mapping_steps = self.mapping_steps + 1\n        \n        mapping_iterations = self.time_steps - mapping_steps + 1\n        self.normalized_data_mapped = np.empty((mapping_iterations, mapping_steps, self.n_columns))\n        \n        for i in range(mapping_iterations):\n            self.normalized_data_mapped[i, :, :] = self.normalized_data[i:i + mapping_steps, :]\n        \n        return self.normalized_data_mapped\n    \n    def rnn_train_test_split(self, test_percent=0.1):\n        \"\"\"\n        Splits the 3D mapped data into training and testing sets for an RNN.\n        \n        Args:\n            test_percent (float): The fraction of data to be used for testing.\n        \n        Returns:\n            tuple: X_train, X_test, y_train, y_test\n        \"\"\"\n        self.test_size = int(np.round(self.normalized_data_mapped.shape[0] * test_percent))\n        self.train_size = self.normalized_data_mapped.shape[0] - self.test_size\n        \n        X_train = self.normalized_data_mapped[:self.train_size, :-1, :] \n        y_train = self.normalized_data_mapped[:self.train_size, -1, :]\n        \n        X_test = self.normalized_data_mapped[self.train_size:, :-1, :]\n        y_test = self.normalized_data_mapped[self.train_size:, -1, :]\n        \n        return X_train, X_test, y_train, y_test  \n\n    def forecast_n_steps(self, model, data: pd.DataFrame, n_forecast_steps=30):\n        \"\"\"\n        Forecast multiple steps ahead using the LSTM model.\n    \n        Args:\n            model (tf.keras.Model): Trained LSTM model for prediction.\n            data (pd.DataFrame): Input DataFrame containing the latest time series data.\n            n_forecast_steps (int): Number of future steps to forecast.\n    \n        Returns:\n            np.array: Forecasted values for n_forecast_steps.\n        \"\"\"\n        # Scaling the latest 'mapping_steps' data for mapping\n        last_steps = self.scaler.transform(data.values)[-self.mapping_steps:]\n    \n        # Initialize normalized_data_mapped array\n        normalized_data_mapped = np.empty((n_forecast_steps, self.mapping_steps, self.n_columns))\n    \n        # Initialize predictions array\n        predictions = np.empty((n_forecast_steps, self.n_columns))\n    \n        # Predict the first step\n        normalized_data_mapped[0, :, :] = last_steps\n        predictions[0, :] = model.predict(\n            normalized_data_mapped[0, :, :].reshape(1, self.mapping_steps, self.n_columns),\n            verbose=False\n        )\n        # Generate predictions and update normalized_data_mapped for each subsequent step\n        for i in range(1, n_forecast_steps):\n            # Shift the window and insert new prediction at end\n            normalized_data_mapped[i, :-1, :] = normalized_data_mapped[i - 1, 1:, :]\n            normalized_data_mapped[i, -1, :] = predictions[i - 1, :]\n    \n            # Predicting next step\n            norm_data = normalized_data_mapped[i, :, :].reshape(1, self.mapping_steps, self.n_columns)\n            predictions[i, :] = model.predict(norm_data, verbose=False)\n    \n        # Inverse transform the predictions to original scale\n        predictions = self.scaler.inverse_transform(predictions)\n        return predictions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing class\nmapping_steps = 32 # ~ 1 months in buisness days\nrnn_formater = RNNFormater(msft[columns], mapping_steps=mapping_steps)\n\n# Mapping steps\nnorm_data_mapped = rnn_formater.data_mapping() # n_steps -> y\n# print(f'Mapped Normalized data step 0:\\n{norm_data_mapped[0]}')\nprint(f'Normalized data shape: {norm_data_mapped[0].shape}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Test Split\nX_train, X_test, y_train, y_test = rnn_formater.rnn_train_test_split(test_percent=0.05)\nprint(f'Number of time steps for test set: {rnn_formater.test_size}')\n\nprint(f'X shape: {X_train.shape}')\n# print(X_train[0])\n\nprint(f'y shape: {y_train.shape}')\n# print(y_train[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vanilla LSTM Model Building","metadata":{}},{"cell_type":"code","source":"# For consistant results\nrandom.seed(0)\nnp.random.seed(0)\ntf.random.set_seed(0)\n\n# Vanilla LSTM\nmodel = models.Sequential([\n    LSTM(units=80, input_shape=(mapping_steps, len(columns))),\n    Dropout(0.05),\n    Dense(units=len(columns))   \n])\n\n# Compiling model\nmodel.compile(optimizer='adam', loss='mae')\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Callback to save model weights\nmodel_checkpoint = ModelCheckpoint('LSTM_Microsoft_model.h5', monitor='val_loss', save_best_only=True)\n\n# Fitting the model\nhistory = model.fit(X_train, y_train, \n                    batch_size=256, \n                    epochs=1_000, \n                    validation_data=(X_test, y_test), \n                    callbacks=[model_checkpoint],\n                    shuffle=False,\n                    verbose=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_training_history(history, plot_title='Training Performance', plot_legends=None, color0=0):\n    \"\"\"\n    Plots the training history of a model using Plotly.\n\n    Args:\n        history (dict): A dictionary containing the training history metrics.\n        plot_title (str): Title of the plot.\n        plot_legends (list): List of legends for the plot. If None, it uses the keys from the history dictionary.\n\n    Returns:\n        None: Displays the plot.\n    \"\"\"\n    # Extracting metrics from the history object\n    epochs = np.arange(1, len(next(iter(history.values()))) + 1)\n    colors = ['blue', 'gold', 'violet', 'lime', 'blue', 'pink', 'yellow']\n    data = []\n\n    # If no legends are provided, use keys from the history\n    if not plot_legends:\n        plot_legends = list(history.keys())\n\n    # Prepare data for each metric in the history\n    for i, (key, legend) in enumerate(zip(history.keys(), plot_legends)):\n        color_index = i % len(colors) + color0\n        data.append(go.Scatter(x=epochs, y=history[key], mode='lines+markers', name=legend, line=dict(color=colors[color_index])))\n\n    # Add error for minimum epoch value\n    min_epoch = np.argmin(history['val_loss']) + 1 \n    loss_str = f\"Train Loss: {history['loss'][min_epoch-1]:.3e}<br>Test Loss: {history['val_loss'][min_epoch - 1]:.3e}\"\n\n    # Creating the layout\n    layout = go.Layout(title=plot_title, xaxis=dict(title='Epochs'), yaxis=dict(title='Value'), width=1100, height=600)\n    fig = go.Figure(data=data, layout=layout)\n\n    # Annotate the minimum loss with an arrow\n    fig.add_annotation(\n        go.layout.Annotation(\n            x=min_epoch,\n            y=history['loss'][min_epoch - 1],\n            xref=\"x\",\n            yref=\"y\",\n            text=loss_str,\n            showarrow=True,\n            arrowhead=7,\n            arrowcolor='green',\n            arrowsize=2,\n            bordercolor='green',\n            borderwidth=2,\n            ax=0,\n            ay=-40\n        )\n    )\n    fig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting LSTM model loss\nplot_training_history(history.history, plot_title='LSTM Model Loss')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def highlight_half(data: pd.DataFrame, axis=1, precision=3):\n    \n    s = data.shape[1] if axis else data.shape[0]\n    data_style = data.style.format(precision=precision)\n\n    def apply_style(val):\n        style1 = 'background-color: red; color: white'\n        style2 = 'background-color: blue; color: white'\n        return [style1 if x < s//2 else style2 for x in range(s)]\n\n    display(data_style.apply(apply_style, axis=axis))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading best wieghts during training\nmodel = models.load_model(f'LSTM_Microsoft_model.h5')\n\n# Predicting\npredictions = model.predict(X_test, verbose=False)\npredictions = rnn_formater.scaler.inverse_transform(predictions)\n\n# Showing predictions and data\nindex_1 = y_test.shape[0]\ndf_y_test = msft[columns].iloc[-index_1:]\ndf_predictions = pd.DataFrame(predictions, index=df_y_test.index, columns=[f'pred_{col}' for col in columns])\ndf_test_pred = pd.concat([df_y_test, df_predictions], axis=1)\n                         \n# Shwoing outputs\ndf_test_pred.tail()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Resdiual Analysis Building","metadata":{}},{"cell_type":"code","source":"# Error dataframe\ndf_error = pd.DataFrame(df_predictions.values - df_y_test.values, index=df.index[-index_1:], columns=[f'error_{col}' for col in columns])\nprint('RMSE Per Column')\nprint((df_error**2).mean()**(1/2))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotly_residual_analysis(df, title_add=''):\n    \"\"\"\n    Perform residual analysis for multiple features in a DataFrame.\n    The DataFrame should contain actual and predicted columns for each feature.\n    \n    Args:\n        df (pd.DataFrame): DataFrame containing actual and predicted columns.\n        title_add (str, optional): Additional title for the subplots.\n    \"\"\"\n    # Number of columns\n    columns = [col for col in df.columns if not col.startswith('pred_')]\n    num_features = len(columns)\n\n    # Color per column\n    colors = ['blue', 'green', 'red', 'purple', 'orange', 'yellow']\n\n    # Subplots per columns\n    fig = make_subplots(rows=num_features, cols=4, vertical_spacing=0.035, horizontal_spacing=0.035,\n                        subplot_titles=(\"Histogram\", \"QQ-Normal Plot\", \"Residuals vs. Predicted Values\", \"Residuals vs Index\"))\n\n    for i, col in enumerate(columns):\n        actual = df[col]\n        predicted = df[f'pred_{col}']\n        residuals = actual - predicted\n        mean_residuals = np.mean(residuals)\n        sd_residuals = np.std(residuals)\n        rmse = np.sqrt(np.mean(residuals**2))\n        index = df.index\n\n        # Assign color for each feature\n        color = colors[i % len(colors)]\n\n        # Histogram of residuals\n        fig.add_trace(go.Histogram(x=residuals, nbinsx=30, name=f'{col.title()} Residuals', marker_color=color), row=i+1, col=1)\n        # Add lines for mean and standard deviation\n        fig.add_vline(x=mean_residuals, line=dict(color='black', width=2), row=i+1, col=1)\n        fig.add_vline(x=mean_residuals + sd_residuals, line=dict(color='grey', width=2, dash='dash'), row=i+1, col=1)\n        fig.add_vline(x=mean_residuals - sd_residuals, line=dict(color='grey', width=2, dash='dash'), row=i+1, col=1)\n        fig.add_annotation(x=mean_residuals, y=5, text=f\"Mean: {mean_residuals:.2f}\", showarrow=True, row=i+1, col=1)\n        fig.add_annotation(x=sd_residuals + mean_residuals, y=5, text=f\"SD: {sd_residuals:.2f}\", showarrow=False, row=i+1, col=1)\n        \n        # QQ-Normal of residuals\n        qq = stats.probplot(residuals, dist=\"norm\", plot=None)\n        fig.add_trace(go.Scatter(x=qq[0][0], y=qq[1][1] + qq[1][0]*qq[0][0], mode='lines',  showlegend=False), row=i+1, col=2)\n        fig.add_trace(go.Scatter(x=qq[0][0], y=qq[0][1], mode='markers', marker_color=color, name=f'{col.title()} QQ'), row=i+1, col=2)\n\n        # Residuals vs. predicted values\n        fig.add_trace(go.Scatter(x=predicted, y=residuals, mode='markers', marker_color=color, name=f'{col.title()} Resid Pred'), row=i+1, col=3)\n        fig.add_hline(y=0, line=dict(color='red'), row=i+1, col=3)\n        fig.add_hline(y=2 * rmse, line=dict(color='red', dash='dash'), row=i+1, col=3)\n        fig.add_hline(y=-2 * rmse, line=dict(color='red', dash='dash'), row=i+1, col=3)\n\n        # Residuals vs. index\n        fig.add_trace(go.Scatter(x=index, y=residuals, mode='markers', marker_color=color, name=f'{col.title()} Resid Index'), row=i+1, col=4)\n        fig.add_hline(y=0, line=dict(color='red'), row=i+1, col=4)\n        fig.add_hline(y=2 * rmse, line=dict(color='red', dash='dash'), row=i+1, col=4)\n        fig.add_hline(y=-2 * rmse, line=dict(color='red', dash='dash'), row=i+1, col=4)\n\n    # Update layout\n    fig.update_layout(height=250*num_features, width=1400, title_text=\"Residual Analysis \" + title_add)\n    fig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Residual Analysis Plot\nplotly_residual_analysis(df_test_pred, \n                         title_add=f'- Microsoft Vanilla LSTM')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Vanila LSTM Predictions","metadata":{}},{"cell_type":"code","source":"def plot_predictions(y_values_df, predictions_df, title_add=''):\n    \"\"\"\n    Plots actual values and predictions for each feature in separate subplots.\n    \n    Args:\n        y_values_df (pd.DataFrame): DataFrame containing actual values.\n        predictions_df (pd.DataFrame): DataFrame containing predicted values.\n        title_add (str, optional): Additional title for the subplots.\n    \"\"\"\n    # Number/color per features \n    columns = [col for col in y_values_df.columns]\n    num_features = len(columns)\n    actual_colors = ['cyan', 'lime', 'yellow', 'violet', 'gold', 'pink']\n\n    # Creating subplots\n    fig = make_subplots(rows=num_features, cols=1, vertical_spacing=0.03, subplot_titles=[col.title() for col in columns])\n\n    for i, col in enumerate(columns):\n        # Actual values trace\n        fig.add_trace(go.Scatter(x=y_values_df.index, y=y_values_df[col], mode='lines', name=col.title(),\n                                 line=dict(color=actual_colors[i % len(actual_colors)])), row=i+1, col=1)\n        \n        # Predicted values trace\n        pred_col = f'pred_{col}'\n        if pred_col in predictions_df.columns:\n            fig.add_trace(go.Scatter(x=predictions_df.index, y=predictions_df[pred_col], \n                                     mode='lines', name=f'Predicted {col.title()}', line=dict(color='red')), row=i+1, col=1)\n            \n            # Calculate RMSE and add as an annotation\n            rmse = np.sqrt(np.mean((y_values_df[col] - predictions_df[pred_col]) ** 2))\n            fig.add_annotation(xref='x domain', yref='y domain', x=1, y=0.05, showarrow=False,\n                               text=f'RMSE: {rmse:.2f}', row=i+1, col=1, font=dict(color='red'))\n    fig.update_layout(height=350*num_features, width=1100, title_text=\"Data & Predictions \" + title_add)\n    fig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting prediction and data\nplot_predictions(df_y_test, df_predictions, title_add=f'- Microsoft Vanilla LSTM')","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510},{"sourceId":8976649,"sourceType":"datasetVersion","datasetId":5405004},{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090,"modelId":735}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune LLaMA2 7b Model with PEFT method for Stock Price Prediction","metadata":{"execution":{"iopub.status.busy":"2024-06-19T09:03:38.177879Z","iopub.execute_input":"2024-06-19T09:03:38.178629Z","iopub.status.idle":"2024-06-19T09:03:38.629074Z","shell.execute_reply.started":"2024-06-19T09:03:38.178583Z","shell.execute_reply":"2024-06-19T09:03:38.627545Z"}}},{"cell_type":"markdown","source":"reference\n- https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis","metadata":{}},{"cell_type":"markdown","source":"As a first step, install the specific libraries necessary to make this work\n- accelerate is a distributed traing library for PyTorch by HugglingFace. it allows you to train your models on mutiple GPU or CPUs in parallel(distributed configurations) which can significatly spped up traing in presense of multiple GPUs(I won't use it in this work.)\n- peft is a python library by HuggingFace for effiecient adaptation of pre-trained language models(PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs.\n- bitsandbytes by Time Dettmers,is a lightweight wrapper around CUDA custom functions,in particular 8-bit optimizers,matrix multiplication(LLM.int8()), and quantization functions.It allows to run models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen(float16,bfloat16,float32, and so on).\n- transformers is a Python library for NLP, it provides a number of pre-trained models for NLP tasks such as text classification, question answering, and machine translation.\n- trl is a full stack library by HuggingFace providing a set of tools to train transfomer language model with Reinforcement Learning, from the Supervised Fine-tuning step(SFT), Reward Modeling step(RM) to the Proximal Policy Optimization(PPO) step. ","metadata":{}},{"cell_type":"markdown","source":"The code imports the os module and sets two environment variables:\n- CUDA_VISIBLE_DEVICES: This environment variables tells PyTorch which GPUs to use. In this case, the code is setting the environment variable to 0, which means that PyTorch will use the first GPU.\n- CUDA_VISIBLE_DEVICES: This environment variable tells the HuggingFace Transfomers library whether to parallelize the tokenization process. In this case, the code is setting the environment variable to false, which means the the tokenization process will not be parallelized.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:10:25.865361Z","iopub.execute_input":"2024-07-19T07:10:25.865845Z","iopub.status.idle":"2024-07-19T07:10:25.876067Z","shell.execute_reply.started":"2024-07-19T07:10:25.865814Z","shell.execute_reply":"2024-07-19T07:10:25.875249Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"- The code import warnings;warnings.fiterwarnings(\"ignore\") imports the warnings module and sets the warning filter to ignore. This means all warnings will be suppressed and will not be displayed. Actually during training there are many warnings that do not prevent the fine-tuning but can be distracting and make you wonder if you are doing the correct things.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"1\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:10:28.902425Z","iopub.execute_input":"2024-07-19T07:10:28.902768Z","iopub.status.idle":"2024-07-19T07:10:28.908273Z","shell.execute_reply.started":"2024-07-19T07:10:28.902741Z","shell.execute_reply":"2024-07-19T07:10:28.907202Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -q -U \"bitsandbytes==0.42.0\"\n!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f\n!pip install git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n!pip install -q -U \"accelerate==0.26.1\"\n!pip install -q -U  \"transformers==4.39.1\"\n!pip install -q -U  \"datasets==2.16.0\"\n!pip install tensorflow[and-cuda]\n!pip3 install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:10:58.138151Z","iopub.execute_input":"2024-07-19T07:10:58.138550Z","iopub.status.idle":"2024-07-19T07:17:44.066654Z","shell.execute_reply.started":"2024-07-19T07:10:58.138519Z","shell.execute_reply":"2024-07-19T07:17:44.065238Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Cloning https://github.com/huggingface/trl.git (to revision 7630f877f91c556d9e5a3baa4b6e2894d90ff84c) to /tmp/pip-req-build-3b0dcelk\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /tmp/pip-req-build-3b0dcelk\n  Running command git rev-parse -q --verify 'sha^7630f877f91c556d9e5a3baa4b6e2894d90ff84c'\n  Running command git fetch -q https://github.com/huggingface/trl.git 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Running command git checkout -q 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Resolved https://github.com/huggingface/trl.git to commit 7630f877f91c556d9e5a3baa4b6e2894d90ff84c\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (2.1.2)\nRequirement already satisfied: transformers>=4.31.0 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (4.42.3)\nRequirement already satisfied: numpy>=1.18.2 in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (1.26.4)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (0.32.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl==0.7.12.dev0) (2.20.0)\nCollecting tyro>=0.5.11 (from trl==0.7.12.dev0)\n  Downloading tyro-0.8.5-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.7.12.dev0) (2024.5.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.23.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.31.0->trl==0.7.12.dev0) (4.66.4)\nCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl==0.7.12.dev0)\n  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.7.12.dev0) (13.7.0)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.7.12.dev0)\n  Downloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl==0.7.12.dev0) (5.9.3)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl==0.7.12.dev0) (3.9.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl==0.7.12.dev0) (4.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.31.0->trl==0.7.12.dev0) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers>=4.31.0->trl==0.7.12.dev0) (2024.7.4)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (2.17.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.7.12.dev0) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl==0.7.12.dev0) (2023.4)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl==0.7.12.dev0) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.7.12.dev0) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.7.12.dev0) (1.16.0)\nDownloading tyro-0.8.5-py3-none-any.whl (103 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.4/103.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\nDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\nBuilding wheels for collected packages: trl\n  Building wheel for trl (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for trl: filename=trl-0.7.12.dev0-py3-none-any.whl size=173434 sha256=1a090c29d7999801f06cc5fd3337cad709b9ebbbf991ba78a710506db37039a5\n  Stored in directory: /root/.cache/pip/wheels/ad/f5/b1/f5ac48230936583c88cfde8596bc92cad4b0a4b24d0f819c06\nSuccessfully built trl\nInstalling collected packages: shtab, docstring-parser, tyro, trl\n  Attempting uninstall: docstring-parser\n    Found existing installation: docstring-parser 0.15\n    Uninstalling docstring-parser-0.15:\n      Successfully uninstalled docstring-parser-0.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed docstring-parser-0.16 shtab-1.7.1 trl-0.7.12.dev0 tyro-0.8.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.6.1 requires cubinlinker, which is not installed.\ncudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.6.1 requires ptxcompiler, which is not installed.\ncuml 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\ncudf 24.6.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.5.1 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ngcsfs 2024.5.0 requires fsspec==2024.5.0, but you have fsspec 2023.10.0 which is incompatible.\npathos 0.3.2 requires dill>=0.3.8, but you have dill 0.3.7 which is incompatible.\npathos 0.3.2 requires multiprocess>=0.70.16, but you have multiprocess 0.70.15 which is incompatible.\nrapids-dask-dependency 24.6.0a0 requires dask==2024.5.1, but you have dask 2024.7.0 which is incompatible.\ns3fs 2024.5.0 requires fsspec==2024.5.0.*, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: tensorflow[and-cuda] in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.2.0)\nRequirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.26.4)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (69.0.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (1.60.0)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.15.0)\nCollecting keras<2.16,>=2.15.0 (from tensorflow[and-cuda])\n  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\nCollecting nvidia-cublas-cu12==12.2.5.6 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.2.5.6-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.2.142 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.2.142-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.4.25 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.4.25-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.8.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.8.103-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.3.141 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.3.141-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.2.141 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.2.141 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.1.2.141-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.16.5 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.16.5-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.2.140 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.2.140-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting tensorrt==8.6.1.post1 (from tensorflow[and-cuda])\n  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting tensorrt-bindings==8.6.1 (from tensorflow[and-cuda])\n  Downloading tensorrt_bindings-8.6.1-cp310-none-manylinux_2_17_x86_64.whl.metadata (621 bytes)\nINFO: pip is looking at multiple versions of tensorflow[and-cuda] to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorflow[and-cuda]\n  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\nCollecting flatbuffers>=24.3.25 (from tensorflow[and-cuda])\n  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\nCollecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow[and-cuda])\n  Downloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (2.32.3)\nCollecting tensorboard<2.18,>=2.17 (from tensorflow[and-cuda])\n  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\nRequirement already satisfied: keras>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow[and-cuda]) (3.4.1)\nCollecting nvidia-cublas-cu12==12.3.4.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cuda-nvcc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.3.107 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cudnn-cu12==8.9.7.29 (from tensorflow[and-cuda])\n  Downloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cufft-cu12==11.0.12.1 (from tensorflow[and-cuda])\n  Downloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.4.107 (from tensorflow[and-cuda])\n  Downloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.5.4.101 (from tensorflow[and-cuda])\n  Downloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.2.0.103 (from tensorflow[and-cuda])\n  Downloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.19.3 (from tensorflow[and-cuda])\n  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12==12.3.101 (from tensorflow[and-cuda])\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.42.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow[and-cuda]) (13.7.0)\nRequirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow[and-cuda]) (0.0.8)\nRequirement already satisfied: optree in /opt/conda/lib/python3.10/site-packages (from keras>=3.2.0->tensorflow[and-cuda]) (0.12.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.7.4)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (3.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow[and-cuda]) (3.1.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow[and-cuda]) (2.1.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow[and-cuda]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.2.0->tensorflow[and-cuda]) (2.17.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow[and-cuda]) (0.1.2)\nDownloading nvidia_cublas_cu12-12.3.4.1-py3-none-manylinux1_x86_64.whl (412.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.6/412.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (14.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvcc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (22.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.0/22.0 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.3.107-py3-none-manylinux1_x86_64.whl (24.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (867 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.7/867.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.7.29-py3-none-manylinux1_x86_64.whl (704.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m704.7/704.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.12.1-py3-none-manylinux1_x86_64.whl (98.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.4.107-py3-none-manylinux1_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.5.4.101-py3-none-manylinux1_x86_64.whl (125.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.2.0.103-py3-none-manylinux1_x86_64.whl (197.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\nDownloading ml_dtypes-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: flatbuffers, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-nvcc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-dtypes, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, tensorflow\n  Attempting uninstall: flatbuffers\n    Found existing installation: flatbuffers 23.5.26\n    Uninstalling flatbuffers-23.5.26:\n      Successfully uninstalled flatbuffers-23.5.26\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.2.0\n    Uninstalling ml-dtypes-0.2.0:\n      Successfully uninstalled ml-dtypes-0.2.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.15.1\n    Uninstalling tensorboard-2.15.1:\n      Successfully uninstalled tensorboard-2.15.1\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.15.0\n    Uninstalling tensorflow-2.15.0:\n      Successfully uninstalled tensorflow-2.15.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ntensorflow-decision-forests 1.8.1 requires tensorflow~=2.15.0, but you have tensorflow 2.17.0 which is incompatible.\ntensorflow-text 2.15.0 requires tensorflow<2.16,>=2.15.0; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.17.0 which is incompatible.\ntf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed flatbuffers-24.3.25 ml-dtypes-0.4.0 nvidia-cublas-cu12-12.3.4.1 nvidia-cuda-cupti-cu12-12.3.101 nvidia-cuda-nvcc-cu12-12.3.107 nvidia-cuda-nvrtc-cu12-12.3.107 nvidia-cuda-runtime-cu12-12.3.101 nvidia-cudnn-cu12-8.9.7.29 nvidia-cufft-cu12-11.0.12.1 nvidia-curand-cu12-10.3.4.107 nvidia-cusolver-cu12-11.5.4.101 nvidia-cusparse-cu12-12.2.0.103 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 tensorboard-2.17.0 tensorflow-2.17.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:18:27.988848Z","iopub.execute_input":"2024-07-19T07:18:27.989746Z","iopub.status.idle":"2024-07-19T07:18:41.242129Z","shell.execute_reply.started":"2024-07-19T07:18:27.989710Z","shell.execute_reply":"2024-07-19T07:18:41.241391Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2024-07-19 07:18:36.845090: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-19 07:18:36.901112: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-19 07:18:36.917255: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"pytorch version {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:18:49.211760Z","iopub.execute_input":"2024-07-19T07:18:49.212623Z","iopub.status.idle":"2024-07-19T07:18:49.217951Z","shell.execute_reply.started":"2024-07-19T07:18:49.212591Z","shell.execute_reply":"2024-07-19T07:18:49.216988Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"pytorch version 2.3.1+cu121\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:18:50.707074Z","iopub.execute_input":"2024-07-19T07:18:50.707482Z","iopub.status.idle":"2024-07-19T07:18:50.712566Z","shell.execute_reply.started":"2024-07-19T07:18:50.707456Z","shell.execute_reply":"2024-07-19T07:18:50.711631Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"working on cuda:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Log in Huggingface","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"Huggingface_token\")\n\nlogin(token = hf_token)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:18:53.515550Z","iopub.execute_input":"2024-07-19T07:18:53.516284Z","iopub.status.idle":"2024-07-19T07:18:53.841501Z","shell.execute_reply.started":"2024-07-19T07:18:53.516252Z","shell.execute_reply":"2024-07-19T07:18:53.840468Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preparing the data and the core evaluation functioins\nThe code in the next cell performs the following steps:\n1. Reads the input dataset from the all-data.csv file, which is a comma-separated value(CSV) file with two columns: sentiment and text.\n2. Splits the dataset into training and test sets,with 300 samples in each set. The split is stratified by sentiment, so that each set contains a representative of positive,neutral, and negative sentiments.\n3. Shuffles the train data in a replicable order(random_state=10)\n4. Transfoms the texts contained in the train and test data into prompts to be used by LLamMa: the train prompts contains the expected answer we want to fine-tune the model-with\n5. The residual examples not in train or test, for reporting purposes during during training (but it won't be used for early stopping), is treated as evaluatio  data, which is sampled with repetition in order to have a 50/50/50 sample (negative instances are very few, hence the shoud be repeated)\n6. The train and eval data are wrapped by the class from HuggingFace's datasets library(backed by the Apache Arrow format)\n\nThis prepares in a single cell train_data, eval_data and test_data datasets to be used in the fine tuning.","metadata":{}},{"cell_type":"code","source":"filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n\ndf = pd.read_csv(filename, \n                 names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")\n\nX_train = list()\nX_test = list()\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=42)\n    X_train.append(train)\n    X_test.append(test)\n\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\n\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)\n\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])\n\ny_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:18:58.597144Z","iopub.execute_input":"2024-07-19T07:18:58.597574Z","iopub.status.idle":"2024-07-19T07:19:00.043937Z","shell.execute_reply.started":"2024-07-19T07:18:58.597545Z","shell.execute_reply":"2024-07-19T07:19:00.043142Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_test","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:19:04.642002Z","iopub.execute_input":"2024-07-19T07:19:04.643048Z","iopub.status.idle":"2024-07-19T07:19:04.658846Z","shell.execute_reply.started":"2024-07-19T07:19:04.643012Z","shell.execute_reply":"2024-07-19T07:19:04.657819Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                                   text\n567   Analyze the sentiment of the news headline enc...\n1752  Analyze the sentiment of the news headline enc...\n995   Analyze the sentiment of the news headline enc...\n601   Analyze the sentiment of the news headline enc...\n568   Analyze the sentiment of the news headline enc...\n...                                                 ...\n4219  Analyze the sentiment of the news headline enc...\n4814  Analyze the sentiment of the news headline enc...\n4059  Analyze the sentiment of the news headline enc...\n4720  Analyze the sentiment of the news headline enc...\n4453  Analyze the sentiment of the news headline enc...\n\n[900 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>567</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>1752</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>601</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>568</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4219</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>4814</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>4059</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>4720</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>4453</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n  </tbody>\n</table>\n<p>900 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Next part to do is creating a function to evaluate the results from the fine-tuned sentiment model. The function performs the following setps\"\n1. Maps the sentiment labels to a numeriacal representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n2. Calculates the accuracy of the model on the test data.\n3. Generates an accuracy report for each sentiment labal.\n4. Generates a classification report for the model.\n5. Generates a confusion matrix for the model.","metadata":{}},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:19:06.950651Z","iopub.execute_input":"2024-07-19T07:19:06.950993Z","iopub.status.idle":"2024-07-19T07:19:06.961015Z","shell.execute_reply.started":"2024-07-19T07:19:06.950967Z","shell.execute_reply":"2024-07-19T07:19:06.960113Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Testing the model without fine-tuning\n\nNext we need to take care of the model, which is a 7b-hf(7 billion parameters, no RLHF(Reinforcement Learning From Human Feedback), in the HuggingFace compatible format), loading from Kaggle models and quantization.\n\nModel loading and quantization:\n- First the code loads the LLaMA2 language model from the HuggingFace Hub.\n- the code gets the float16 type from the torch library. This is the data type that will be used for the computations.\n- Next, it creates a BitsAndBytesConfig object with the following setting:\n    1. load_in_4bit: Load the model weights in 4-bit format.\n    2. bnb-4bit-quant_type: Use the \"nf4\" quantization type. 4-bit NormalFloat(NF4),is a new data type that is information theoretically optimal for normally distributed weights.\n    3. bnb_4bit_compute_dtype: Use the float16 data type for computations.\n    4. bnb_4bit_use_double_quant: Do not use double quantization(reduces the average memory footprint by quantizing also the quantization constants and saves an additional 0.4 bits per parameter.)\n- Then the code creates a AutoModelForCasualLM object from the pre-trained LLaMA2 language model, using the BitAndBytesConfig object for quantization.\n- After that, the code disables caching for model.\n- Finally the code sets the pre-training token probability to 1.\n\nTokenizer loading:\n- First, the code loads the tokenizer for the LLaMA2 language model.\n- Then it sets the padding token to be the end-of-sequnce(EOS) token.\n- Finally, the code sets the padding side to be \"right\",which means that the inpus sequences will be padded on the right side. this is crucial for correct padding direction (this is the way with LLaMA2).","metadata":{}},{"cell_type":"markdown","source":"#### docs of BitsAndByteConfig(https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig)\n- load_in_4bit (bool, optional, defaults to False) — This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4(4-bit floating-point)/NF4((normalized float 4) layers from bitsandbytes.\n- bnb_4bit_quant_type (str, optional, defaults to \"fp4\") — This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types which are specified by fp4 or nf4.\n- bnb_4bit_compute_dtype (torch.dtype or str, optional, defaults to torch.float32) — This sets the computational type which might be different than the input type. For example, inputs might be fp32, but computation can be set to bf16 for speedups.\n-bnb_4bit_use_double_quant (bool, optional, defaults to False) — This flag is used for nested quantization where the quantization constants from the first quantization are quantized again.\n\n#### AutoModelForCausalLM\nhttps://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM","metadata":{}},{"cell_type":"code","source":"model_name = \"../input/llama-2/pytorch/7b-hf/1\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:19:10.110339Z","iopub.execute_input":"2024-07-19T07:19:10.111278Z","iopub.status.idle":"2024-07-19T07:21:47.304002Z","shell.execute_reply.started":"2024-07-19T07:19:10.111229Z","shell.execute_reply":"2024-07-19T07:21:47.302972Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f9747355eac4cdbbd99cd5c7b0a28cc"}},"metadata":{}}]},{"cell_type":"markdown","source":"In the next cell, we set a function for predicting the sentiment of a neas headline using the LLaMA2 language model. The function takes three arguments.\n\ntest: a Pandas DataFrame containing the news headlines to be predicted. model: The pre-trained LLaMA2 language model. tokenizer: The tokenizer for the LLaMA2 language model.\n\nThe function works as follows:\n1. For each news headling in the test DataFrame:\n    - Create a prompt for the language model, which asks it to analyze the sentiment of the news headline and return the corresponding sentiment label.\n    - Use the pipeline() function from HuggingFace Transformers library to generate text from the language model, using the prompt.\n    - Extract the predicted sentiment label from the generated text.\n    - Append the predicted sentiment label to the y-pred list.\n2. Return the y_pred list\n\n- The pipeline() function from the HuggingFace Transformers library is used to generate text from the language model.The task argument specifies that the task is text generation. The model and tokenizer argument specify the pre-trained LLaMa2 language model and the tokenizer for the language model. The max_new_tokens argument specifies the maximum number of new tokens to generate. The teperature argument controls the randomness of the generated text. A lower temperature will produce mode predictable text, while a higher temperature will produce more creative and unexpected text.\n\n- The if statement checks if the generated text contains the word \"positive\". If it does then the predicted sentiment label is \"positive\". Otherwise, the if statement checks if the generated text contains the word \"negative\". If it does, the the predicted sentiment label is \"negative\".  Otherwise, the if statement checks if the generated text contains the word \"neutral\". If it does, the the predicted sentiment label is \"neutral\".","metadata":{}},{"cell_type":"code","source":"def predict(test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(X_test))):\n        prompt = X_test.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens = 1, \n                        temperature = 0.0,\n                       )\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:21:54.133127Z","iopub.execute_input":"2024-07-19T07:21:54.133515Z","iopub.status.idle":"2024-07-19T07:21:54.140961Z","shell.execute_reply.started":"2024-07-19T07:21:54.133480Z","shell.execute_reply":"2024-07-19T07:21:54.139999Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"At this point, we are ready to test the LLaMA2 7b-hf model and see how it performs on this problem without any fine-tuning. This allows to get insights on the model itself and establish a baseline.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(test,model,tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:21:58.101447Z","iopub.execute_input":"2024-07-19T07:21:58.101813Z","iopub.status.idle":"2024-07-19T07:27:27.599798Z","shell.execute_reply.started":"2024-07-19T07:21:58.101784Z","shell.execute_reply":"2024-07-19T07:27:27.598856Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [05:29<00:00,  2.73it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the following cell, we evaluate the results. There is little to be said, it is performing really terribly because the 7b-hf model tends to just predict a neutral sentiment and seldom it detects positive or negative sentiment.","metadata":{}},{"cell_type":"code","source":"evaluate(y_true,y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:27:44.283480Z","iopub.execute_input":"2024-07-19T07:27:44.283863Z","iopub.status.idle":"2024-07-19T07:27:44.310505Z","shell.execute_reply.started":"2024-07-19T07:27:44.283833Z","shell.execute_reply":"2024-07-19T07:27:44.309491Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Accuracy: 0.373\nAccuracy for label 0: 0.027\nAccuracy for label 1: 0.937\nAccuracy for label 2: 0.157\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.89      0.03      0.05       300\n           1       0.34      0.94      0.50       300\n           2       0.67      0.16      0.25       300\n\n    accuracy                           0.37       900\n   macro avg       0.63      0.37      0.27       900\nweighted avg       0.63      0.37      0.27       900\n\n\nConfusion Matrix:\n[[  8 287   5]\n [  1 281  18]\n [  0 253  47]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-tuning\n\nIn the next cell we set everything ready. for the fine-tuning, We configures and initializes a Simple Fine-tuning Trainer(SFTTrainer) for training a large language model using the PEFT method, which should save time as it operates on a reduced number of parameters compared to the model's overall size. The PEFT method focuses on refining a limited set of (additional) model parameters, while keeping the majority of the pre-trained LLM parameters fixed. This signifucatly reduces both computatioanl and storage expenses. Additionally, this strategy addresses the challenge of catastrophic forgetting, which often occurs during the coplete fine-tuning of LLMs/\n\n### PEFTConfig:\n\nThe peft_config object specified the parameters for PEFT. THe following are some of most important parameters:\n\n- lora_alpha: The learning rate for the LoRA update metrices.\n- lora_dropout: The dropout probability for the LoRA updata matrices.\n- r: The rank of the LoRA update matrics.\n- bias: The type of bias to use. The possible values are none,additive, and learned.\n- task_type: The type of task that the model is being trained for, The possible valuse are CAUSAL_LM and MASKED_LM.\n\n### TrainingArguments:\n\nThe traing arguments object specifies the parameters for training the model. The following are some of the most important parameters:\n\n- output_dir: The directory where the training logs and checkpoints will be saved.\n- num_train_epochs: The number of epochs to train the model for.\n- per_device_train_batch_size: The number of samples in each batch on each device.\n- gradient_accumulation_steps: The number of batches to accumulate gradients before updating the model parameters.\n- optim: The optimizer to use for training the model.\n- save_steps: The number of steps after which to save a checkpoint.\n- logging_steps: The number of steps after which to log the training metrics.\n- learning_rate: The learning rate for the optimizer.\n- weight_decay: The weight decay parameter for the optimizer.\n- fp16: Whether to use 16-bit floating-point precision.\n- bf16: Whether to use BFloat16 precision.\n- max_grad_norm: The maximum gradient norm.\n- max_steps: The maximum number of steps to train the model for.\n- warmup_ratio: The proportion of the training steps to use for warming up the learning rate.\n- group_by_length: Whether to group the training samples by length.\n- lr_scheduler_type: The type of learning rate scheduler to use.\n- report_to: The tools to report the training metrics to.\n- evaluation_strategy: The strategy for evaluating the model during training. \n\n### SFTTrainer:\n\nThe SFTTrainer is a custom trainer class from the TRL library. It is used to train large language models (also using the PEFT method).\n\nThe SFTTrainer object is initialized with the following arguments:\n- model: The model to be trained.\n- train_dataset: The training dataset.\n- eval_dataset: The evaluation dataset.\n- peft_config: The PEFT configuration.\n- dataset_text_field: The name of the text field in the dataset.\n- tokenizer: The tokenizer to use.\n- args: The training arguments.\n- packing: Whether to pack the training samples.\n- max_seq_length: The maximum sequence length.\n\nOnce the SFTTrainer object is intialized, it can be used to train the model by calling the train() method.","metadata":{}},{"cell_type":"code","source":"output_dir=\"fined_tuned_llama2_for_finance_sentiment\"\n\npeft_config = LoraConfig(\n        lora_alpha=16, \n        lora_dropout=0.1,\n        r=64,\n        bias=\"none\",\n        target_modules=\"all-linear\",\n        task_type=\"CAUSAL_LM\",\n)\n\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,                    # directory to save and repository id\n    num_train_epochs=3,                       # number of training epochs\n    per_device_train_batch_size=1,            # batch size per device during training\n    gradient_accumulation_steps=8,            # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n    optim=\"paged_adamw_32bit\",\n    save_steps=0,\n    logging_steps=25,                         # log every 10 steps\n    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n    weight_decay=0.001,\n    fp16=True,\n    bf16=False,\n    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n    max_steps=-1,\n    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n    group_by_length=True,\n    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n    report_to=\"tensorboard\",                  # report metrics to tensorboard\n    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n)\n\ntrainer = SFTTrainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    max_seq_length=1024,\n    packing=False,\n    dataset_kwargs={\n        \"add_special_tokens\": False,\n        \"append_concat_token\": False,\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:28:46.255371Z","iopub.execute_input":"2024-07-19T07:28:46.256329Z","iopub.status.idle":"2024-07-19T07:28:49.318666Z","shell.execute_reply.started":"2024-07-19T07:28:46.256277Z","shell.execute_reply":"2024-07-19T07:28:49.317894Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b5a9c85062d402c80a8f14d2ba6fe70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a43c42817c6423ba94308d873e7c1b2"}},"metadata":{}}]},{"cell_type":"markdown","source":"The following code will train the model using the trainer.train() method and then save the trained model to the trained model to the trained-model directory.","metadata":{}},{"cell_type":"markdown","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.10,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])\n# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device0) \n#Load weights\nmodel_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_0.eval()\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device1)\nmodel_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_1.eval()\n\n#Trainable Parameters\nmodel_0.print_trainable_parameters(), model_1.print_trainable_parameters()","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T07:28:57.705073Z","iopub.execute_input":"2024-07-19T07:28:57.705932Z","iopub.status.idle":"2024-07-19T08:30:03.706058Z","shell.execute_reply.started":"2024-07-19T07:28:57.705892Z","shell.execute_reply":"2024-07-19T08:30:03.705271Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [336/336 1:00:51, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.802600</td>\n      <td>0.698949</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.517000</td>\n      <td>0.713908</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=336, training_loss=0.7177914522943043, metrics={'train_runtime': 3665.3631, 'train_samples_per_second': 0.737, 'train_steps_per_second': 0.092, 'total_flos': 1.0717884041527296e+16, 'train_loss': 0.7177914522943043, 'epoch': 2.99})"},"metadata":{}}]},{"cell_type":"markdown","source":"training output of 1st trial : TrainOutput(global_step=336, training_loss=0.7172041322503772, metrics={'train_runtime': 3666.0185, 'train_samples_per_second': 0.736, 'train_steps_per_second': 0.092, 'total_flos': 1.0717884041527296e+16, 'train_loss': 0.7172041322503772, 'epoch': 2.99})\nTrainOutput(global_step=336, training_loss=0.7177914522943043, metrics={'train_runtime': 3665.3631, 'train_samples_per_second': 0.737, 'train_steps_per_second': 0.092, 'total_flos': 1.0717884041527296e+16, 'train_loss': 0.7177914522943043, 'epoch': 2.99})","metadata":{}},{"cell_type":"markdown","source":"The model and the tokenizer are saved to disk for later usage.","metadata":{}},{"cell_type":"code","source":"# Save trained model and tokenizer\ntrainer.save_model()\ntokenizer.save_pretrained(output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T08:30:26.954553Z","iopub.execute_input":"2024-07-19T08:30:26.954921Z","iopub.status.idle":"2024-07-19T08:30:31.354154Z","shell.execute_reply.started":"2024-07-19T08:30:26.954891Z","shell.execute_reply":"2024-07-19T08:30:31.353011Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('fined_tuned_llama2_for_finance_sentiment/tokenizer_config.json',\n 'fined_tuned_llama2_for_finance_sentiment/special_tokens_map.json',\n 'fined_tuned_llama2_for_finance_sentiment/tokenizer.model',\n 'fined_tuned_llama2_for_finance_sentiment/added_tokens.json',\n 'fined_tuned_llama2_for_finance_sentiment/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"### Save fine-tuned model to Huggingface","metadata":{}},{"cell_type":"code","source":"# trainer.model.save_pretrained(output_dir)\ntrainer.model.push_to_hub(output_dir, use_temp_dir=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T08:30:52.880971Z","iopub.execute_input":"2024-07-19T08:30:52.881699Z","iopub.status.idle":"2024-07-19T08:31:45.486376Z","shell.execute_reply.started":"2024-07-19T08:30:52.881665Z","shell.execute_reply":"2024-07-19T08:31:45.485473Z"},"trusted":true},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cdd2624b76493693b7db32e2a449bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef57ec17b1794e1b8bbf2190cd7b6409"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Journee-Kim/fined_tuned_llama2_for_finance_sentiment/commit/d4b080f5a232ff5efc9060ac7f146ea9ec3b613b', commit_message='Upload model', commit_description='', oid='d4b080f5a232ff5efc9060ac7f146ea9ec3b613b', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"Afterwards, loading the TensorBoard extension and start TensorBoard, pointing to the logs/runs directory, which is assumed to contain the training logs and checkpoints for your model, will allow you to understand how the models fits during the training.","metadata":{}},{"cell_type":"markdown","source":"# Saving model to disk for later usage\n\nAt this point, in order to demonstrate how to re-utilize the model, we reload it from the disk and merge it with the original LLaMA model.\n\nIn factm when working with QLoRA, we exclusively train adapters instead of the entire model. So, when you save the model during training, you're only preserving the adapter weights, not the entire model. If you can merge the adapter weights into the model weights using the merge_and_upload method. Then, you can save the model using the model using the save_pretrained method. This will create a default model that's ready for inference tasks.\n\nBefore proceeding, we first remove the previous model and clean up the memory from various onjects we won't use anymore.","metadata":{}},{"cell_type":"code","source":"import gc\ndel [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\ndel [df, X_train, X_eval]\ndel [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:29:41.102440Z","iopub.execute_input":"2024-07-19T09:29:41.102829Z","iopub.status.idle":"2024-07-19T09:29:41.172568Z","shell.execute_reply.started":"2024-07-19T09:29:41.102797Z","shell.execute_reply":"2024-07-19T09:29:41.171326Z"},"trusted":true},"execution_count":104,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[104], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m [model, tokenizer, peft_config, trainer, train_data, eval_data, bnb_config, training_arguments]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m [df, X_train, X_eval]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\n","\u001b[0;31mNameError\u001b[0m: name 'peft_config' is not defined"],"ename":"NameError","evalue":"name 'peft_config' is not defined","output_type":"error"}]},{"cell_type":"code","source":"for _ in range(100):\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:29:43.111088Z","iopub.execute_input":"2024-07-19T09:29:43.112181Z","iopub.status.idle":"2024-07-19T09:30:18.313598Z","shell.execute_reply.started":"2024-07-19T09:29:43.112120Z","shell.execute_reply":"2024-07-19T09:30:18.312604Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:29:33.911184Z","iopub.execute_input":"2024-07-19T09:29:33.911554Z","iopub.status.idle":"2024-07-19T09:29:35.022347Z","shell.execute_reply.started":"2024-07-19T09:29:33.911523Z","shell.execute_reply":"2024-07-19T09:29:35.021347Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"Fri Jul 19 09:29:34 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0             32W /  250W |   14217MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then we can proceed to merging the weights and we will be using the merged model for our purposes.","metadata":{}},{"cell_type":"code","source":"from peft import AutoPeftModelForCausalLM\n\nfinetuned_model = \"./fined_tuned_llama2_for_finance_sentiment/\"\ncompute_dtype = getattr(torch,'float16')\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama-2/pytorch/7b-hf/1\")\n\nmodel =  AutoPeftModelForCausalLM.from_pretrained(\n     finetuned_model,\n     torch_dtype=compute_dtype,\n     return_dict=False,\n     low_cpu_mem_usage=True,\n     device_map=device,\n)\n\nmerged_model = model.merge_and_unload()\nmerged_model.save_pretrained(\"./merged_model\",safe_serialization=True, max_shard_size=\"2GB\")\ntokenizer.save_pretrained(\"./merged_model\")","metadata":{"execution":{"iopub.status.busy":"2024-07-19T08:33:15.855432Z","iopub.execute_input":"2024-07-19T08:33:15.856369Z","iopub.status.idle":"2024-07-19T08:34:15.262691Z","shell.execute_reply.started":"2024-07-19T08:33:15.856333Z","shell.execute_reply":"2024-07-19T08:34:15.261718Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63f662626c04780a2063de4ec933f8f"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nSome non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 4096}\nYour generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"('./merged_model/tokenizer_config.json',\n './merged_model/special_tokens_map.json',\n './merged_model/tokenizer.model',\n './merged_model/added_tokens.json',\n './merged_model/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"#save a merged model to huggingface\nmerged_model.push_to_hub(\"fined_tuned_llama2_for_finance_sentiment\", use_temp_dir=True)\ntokenizer.push_to_hub(\"fined_tuned_llama2_for_finance_sentiment\", use_temp_dir=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:30:25.394627Z","iopub.execute_input":"2024-07-19T09:30:25.395453Z","iopub.status.idle":"2024-07-19T09:34:36.949694Z","shell.execute_reply.started":"2024-07-19T09:30:25.395421Z","shell.execute_reply":"2024-07-19T09:34:36.948439Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 4096}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"482529d7f8ca403c91b7c310d15f6203"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"854d13ac3482447f971bfe65345a4513"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a90d6f71cf5a457c95bb3eda24f2124d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68bf694bfda84a3fb98b90db01c95f9c"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[106], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#save a merged model to huggingface\u001b[39;00m\n\u001b[1;32m      2\u001b[0m merged_model\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfined_tuned_llama2_for_finance_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mpush_to_hub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfined_tuned_llama2_for_finance_sentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_temp_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"],"ename":"NameError","evalue":"name 'tokenizer' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"# Testing \n\nThe following code will first predict the sentiment labels for the test set using the predict() function. Then, it will evaluate the model's perfomance on the test set using the evaluate() function. The result now should be impressive with an overall accuracy of over 0.8 and high accuracy, precision and recall for the single sentiment labels. The prediction of the neutral label can still be improved, yet it is impressive how much could be done with little data and some fine-tuning.","metadata":{}},{"cell_type":"code","source":"y_pred = predict(test, merged_model, tokenizer)\nevaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T08:34:38.039535Z","iopub.execute_input":"2024-07-19T08:34:38.040238Z","iopub.status.idle":"2024-07-19T08:38:28.258641Z","shell.execute_reply.started":"2024-07-19T08:34:38.040195Z","shell.execute_reply":"2024-07-19T08:38:28.257671Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"100%|██████████| 900/900 [03:50<00:00,  3.91it/s]","output_type":"stream"},{"name":"stdout","text":"Accuracy: 0.853\nAccuracy for label 0: 0.913\nAccuracy for label 1: 0.863\nAccuracy for label 2: 0.783\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.91      0.93       300\n           1       0.75      0.86      0.80       300\n           2       0.88      0.78      0.83       300\n\n    accuracy                           0.85       900\n   macro avg       0.86      0.85      0.85       900\nweighted avg       0.86      0.85      0.85       900\n\n\nConfusion Matrix:\n[[274  24   2]\n [ 11 259  30]\n [  3  62 235]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The following code will create a Pandas DataFrame called evaluation containing the text,true labels, and predicted labels from the test set. This is expectially useful for understanding the errors that the fine-tuned model makes, and getting insights on how to improve the prompt.","metadata":{}},{"cell_type":"code","source":"evaluation = pd.DataFrame({'text': X_test[\"text\"], \n                           'y_true':y_true, \n                           'y_pred': y_pred},\n                         )\nevaluation.to_csv(\"test_predictions.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T08:42:26.623676Z","iopub.execute_input":"2024-07-19T08:42:26.624025Z","iopub.status.idle":"2024-07-19T08:42:26.650393Z","shell.execute_reply.started":"2024-07-19T08:42:26.623997Z","shell.execute_reply":"2024-07-19T08:42:26.649576Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Sentiment Analysis of Microsoft and Tesla News Articles","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nfilename = \"../input/microsoft-tesla-finance-news-articles2020-2024/tsla_articles.csv\"\n\ntsla = pd.read_csv(filename,\n                 encoding=\"utf-8\", encoding_errors=\"replace\",index_col=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:17.373379Z","iopub.execute_input":"2024-07-19T09:17:17.373714Z","iopub.status.idle":"2024-07-19T09:17:17.388205Z","shell.execute_reply.started":"2024-07-19T09:17:17.373688Z","shell.execute_reply":"2024-07-19T09:17:17.387418Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"tsla","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:19.366356Z","iopub.execute_input":"2024-07-19T09:17:19.366954Z","iopub.status.idle":"2024-07-19T09:17:19.377105Z","shell.execute_reply.started":"2024-07-19T09:17:19.366926Z","shell.execute_reply":"2024-07-19T09:17:19.376212Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"            date                                               text\n0     2021-01-27  The company benefited from a jump in sales of ...\n1     2021-01-19  Rivian, which has raised another $2.65 billion...\n2     2021-01-15  Traditional automakers have struggled to sell ...\n3     2021-01-22  The pandemic dampened sales for all automakers...\n4     2021-01-07  Mr. Musk’s net worth was $188.5 billion at 10:...\n...          ...                                                ...\n1409  2020-12-02  The president-elect’s economic team says that ...\n1410  2020-12-28  Mike Strizki powers his house and cars with hy...\n1411  2020-12-09  A panel of experts debate what post-Covid poli...\n1412  2020-12-26  Investors of all stripes piled into stocks thi...\n1413  2020-12-01  It will ask the S.E.C. to approve a new rule r...\n\n[1414 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-01-27</td>\n      <td>The company benefited from a jump in sales of ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-01-19</td>\n      <td>Rivian, which has raised another $2.65 billion...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-01-15</td>\n      <td>Traditional automakers have struggled to sell ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-01-22</td>\n      <td>The pandemic dampened sales for all automakers...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-01-07</td>\n      <td>Mr. Musk’s net worth was $188.5 billion at 10:...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1409</th>\n      <td>2020-12-02</td>\n      <td>The president-elect’s economic team says that ...</td>\n    </tr>\n    <tr>\n      <th>1410</th>\n      <td>2020-12-28</td>\n      <td>Mike Strizki powers his house and cars with hy...</td>\n    </tr>\n    <tr>\n      <th>1411</th>\n      <td>2020-12-09</td>\n      <td>A panel of experts debate what post-Covid poli...</td>\n    </tr>\n    <tr>\n      <th>1412</th>\n      <td>2020-12-26</td>\n      <td>Investors of all stripes piled into stocks thi...</td>\n    </tr>\n    <tr>\n      <th>1413</th>\n      <td>2020-12-01</td>\n      <td>It will ask the S.E.C. to approve a new rule r...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1414 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def generate_sentiment_analysis_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:23.000365Z","iopub.execute_input":"2024-07-19T09:17:23.000724Z","iopub.status.idle":"2024-07-19T09:17:23.008636Z","shell.execute_reply.started":"2024-07-19T09:17:23.000695Z","shell.execute_reply":"2024-07-19T09:17:23.007465Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"article_input = pd.DataFrame(tsla.apply(generate_sentiment_analysis_prompt, axis=1), \n                       columns=[\"text\"])","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:26.458451Z","iopub.execute_input":"2024-07-19T09:17:26.459049Z","iopub.status.idle":"2024-07-19T09:17:26.479534Z","shell.execute_reply.started":"2024-07-19T09:17:26.459017Z","shell.execute_reply":"2024-07-19T09:17:26.478578Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"article_input","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:29.962768Z","iopub.execute_input":"2024-07-19T09:17:29.963099Z","iopub.status.idle":"2024-07-19T09:17:29.973060Z","shell.execute_reply.started":"2024-07-19T09:17:29.963074Z","shell.execute_reply":"2024-07-19T09:17:29.972092Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"                                                   text\n0     Analyze the sentiment of the news headline enc...\n1     Analyze the sentiment of the news headline enc...\n2     Analyze the sentiment of the news headline enc...\n3     Analyze the sentiment of the news headline enc...\n4     Analyze the sentiment of the news headline enc...\n...                                                 ...\n1409  Analyze the sentiment of the news headline enc...\n1410  Analyze the sentiment of the news headline enc...\n1411  Analyze the sentiment of the news headline enc...\n1412  Analyze the sentiment of the news headline enc...\n1413  Analyze the sentiment of the news headline enc...\n\n[1414 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1409</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>1410</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>1411</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>1412</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n    <tr>\n      <th>1413</th>\n      <td>Analyze the sentiment of the news headline enc...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1414 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def get_sentiment_score(test, model, tokenizer):\n    y_pred = []\n    for i in tqdm(range(len(article_input))):\n        prompt = article_input.iloc[i][\"text\"]\n        pipe = pipeline(task=\"text-generation\", \n                        model=model, \n                        tokenizer=tokenizer, \n                        max_new_tokens = 1, \n                        temperature = 0.0,\n                       )\n        result = pipe(prompt)\n        answer = result[0]['generated_text'].split(\"=\")[-1]\n        if \"positive\" in answer:\n            y_pred.append(\"positive\")\n        elif \"negative\" in answer:\n            y_pred.append(\"negative\")\n        elif \"neutral\" in answer:\n            y_pred.append(\"neutral\")\n        else:\n            y_pred.append(\"none\")\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:48.450717Z","iopub.execute_input":"2024-07-19T09:17:48.451077Z","iopub.status.idle":"2024-07-19T09:17:48.458479Z","shell.execute_reply.started":"2024-07-19T09:17:48.451048Z","shell.execute_reply":"2024-07-19T09:17:48.457557Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"code","source":"sentiment_labels = get_sentiment_score(test, merged_model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:17:56.601312Z","iopub.execute_input":"2024-07-19T09:17:56.601673Z","iopub.status.idle":"2024-07-19T09:24:38.409737Z","shell.execute_reply.started":"2024-07-19T09:17:56.601645Z","shell.execute_reply":"2024-07-19T09:24:38.408792Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stderr","text":"100%|██████████| 1414/1414 [06:41<00:00,  3.52it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"labels_df = pd.DataFrame(sentiment_labels, columns=['label'])\ntsla['label'] = labels_df\ntsla.to_csv(\"tsla_with_sentiment_label.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:26:01.059593Z","iopub.execute_input":"2024-07-19T09:26:01.060467Z","iopub.status.idle":"2024-07-19T09:26:01.083571Z","shell.execute_reply.started":"2024-07-19T09:26:01.060434Z","shell.execute_reply":"2024-07-19T09:26:01.082536Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"len(sentiment_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:28:37.169311Z","iopub.execute_input":"2024-07-19T09:28:37.169648Z","iopub.status.idle":"2024-07-19T09:28:37.176069Z","shell.execute_reply.started":"2024-07-19T09:28:37.169622Z","shell.execute_reply":"2024-07-19T09:28:37.174953Z"},"trusted":true},"execution_count":102,"outputs":[{"execution_count":102,"output_type":"execute_result","data":{"text/plain":"1414"},"metadata":{}}]},{"cell_type":"code","source":"tsla","metadata":{"execution":{"iopub.status.busy":"2024-07-19T09:26:03.978310Z","iopub.execute_input":"2024-07-19T09:26:03.979128Z","iopub.status.idle":"2024-07-19T09:26:03.994664Z","shell.execute_reply.started":"2024-07-19T09:26:03.979081Z","shell.execute_reply":"2024-07-19T09:26:03.993615Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"            date                                               text     label\n0     2021-01-27  The company benefited from a jump in sales of ...  negative\n1     2021-01-19  Rivian, which has raised another $2.65 billion...   neutral\n2     2021-01-15  Traditional automakers have struggled to sell ...   neutral\n3     2021-01-22  The pandemic dampened sales for all automakers...  negative\n4     2021-01-07  Mr. Musk’s net worth was $188.5 billion at 10:...  positive\n...          ...                                                ...       ...\n1409  2020-12-02  The president-elect’s economic team says that ...   neutral\n1410  2020-12-28  Mike Strizki powers his house and cars with hy...   neutral\n1411  2020-12-09  A panel of experts debate what post-Covid poli...   neutral\n1412  2020-12-26  Investors of all stripes piled into stocks thi...   neutral\n1413  2020-12-01  It will ask the S.E.C. to approve a new rule r...   neutral\n\n[1414 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2021-01-27</td>\n      <td>The company benefited from a jump in sales of ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2021-01-19</td>\n      <td>Rivian, which has raised another $2.65 billion...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2021-01-15</td>\n      <td>Traditional automakers have struggled to sell ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2021-01-22</td>\n      <td>The pandemic dampened sales for all automakers...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2021-01-07</td>\n      <td>Mr. Musk’s net worth was $188.5 billion at 10:...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1409</th>\n      <td>2020-12-02</td>\n      <td>The president-elect’s economic team says that ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1410</th>\n      <td>2020-12-28</td>\n      <td>Mike Strizki powers his house and cars with hy...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1411</th>\n      <td>2020-12-09</td>\n      <td>A panel of experts debate what post-Covid poli...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1412</th>\n      <td>2020-12-26</td>\n      <td>Investors of all stripes piled into stocks thi...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1413</th>\n      <td>2020-12-01</td>\n      <td>It will ask the S.E.C. to approve a new rule r...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n<p>1414 rows × 3 columns</p>\n</div>"},"metadata":{}}]}]}
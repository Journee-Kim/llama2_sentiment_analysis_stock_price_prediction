{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1192499,"sourceType":"datasetVersion","datasetId":622510},{"sourceId":4295,"sourceType":"modelInstanceVersion","modelInstanceId":3090}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-tune LLaMA2 7b Model with PEFT method for Stock Price Prediction","metadata":{"execution":{"iopub.status.busy":"2024-06-19T09:03:38.177879Z","iopub.execute_input":"2024-06-19T09:03:38.178629Z","iopub.status.idle":"2024-06-19T09:03:38.629074Z","shell.execute_reply.started":"2024-06-19T09:03:38.178583Z","shell.execute_reply":"2024-06-19T09:03:38.627545Z"}}},{"cell_type":"markdown","source":"reference\n- https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis","metadata":{}},{"cell_type":"markdown","source":"As a first step, install the specific libraries necessary to make this work\n- accelerate is a distributed traing library for PyTorch by HugglingFace. it allows you to train your models on mutiple GPU or CPUs in parallel(distributed configurations) which can significatly spped up traing in presense of multiple GPUs(I won't use it in this work.)\n- peft is a python library by HuggingFace for effiecient adaptation of pre-trained language models(PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT methods only fine-tune a small number of (extra) model parameters, thereby greatly decreasing the computational and storage costs.\n- bitsandbytes by Time Dettmers,is a lightweight wrapper around CUDA custom functions,in particular 8-bit optimizers,matrix multiplication(LLM.int8()), and quantization functions.It allows to run models stored in 4-bit precision: while 4-bit bitsandbytes stores weights in 4-bits, the computation still happens in 16 or 32-bit and here any combination can be chosen(float16,bfloat16,float32, and so on).\n- transformers is a Python library for NLP, it provides a number of pre-trained models for NLP tasks such as text classification, question answering, and machine translation.\n- trl is a full stack library by HuggingFace providing a set of tools to train transfomer language model with Reinforcement Learning, from the Supervised Fine-tuning step(SFT), Reward Modeling step(RM) to the Proximal Policy Optimization(PPO) step. ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T08:41:39.363973Z","iopub.execute_input":"2024-06-21T08:41:39.364484Z","iopub.status.idle":"2024-06-21T08:41:40.011407Z","shell.execute_reply.started":"2024-06-21T08:41:39.364412Z","shell.execute_reply":"2024-06-21T08:41:40.009333Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_66Agree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_AllAgree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/README.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/License.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_75Agree.txt\n/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_50Agree.txt\n/kaggle/input/llama-2/pytorch/7b-hf/1/model.safetensors.index.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/config.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/model-00001-of-00002.safetensors\n/kaggle/input/llama-2/pytorch/7b-hf/1/Responsible-Use-Guide.pdf\n/kaggle/input/llama-2/pytorch/7b-hf/1/model-00002-of-00002.safetensors\n/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model-00002-of-00002.bin\n/kaggle/input/llama-2/pytorch/7b-hf/1/README.md\n/kaggle/input/llama-2/pytorch/7b-hf/1/USE_POLICY.md\n/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer_config.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model.bin.index.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/LICENSE.txt\n/kaggle/input/llama-2/pytorch/7b-hf/1/pytorch_model-00001-of-00002.bin\n/kaggle/input/llama-2/pytorch/7b-hf/1/special_tokens_map.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/.gitattributes\n/kaggle/input/llama-2/pytorch/7b-hf/1/tokenizer.model\n/kaggle/input/llama-2/pytorch/7b-hf/1/added_tokens.json\n/kaggle/input/llama-2/pytorch/7b-hf/1/generation_config.json\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code imports the os module and sets two environment variables:\n- CUDA_VISIBLE_DEVICES: This environment variables tells PyTorch which GPUs to use. In this case, the code is setting the environment variable to 0, which means that PyTorch will use the first GPU.\n- CUDA_VISIBLE_DEVICES: This environment variable tells the HuggingFace Transfomers library whether to parallelize the tokenization process. In this case, the code is setting the environment variable to false, which means the the tokenization process will not be parallelized.","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:03:39.094847Z","iopub.execute_input":"2024-06-20T11:03:39.095222Z","iopub.status.idle":"2024-06-20T11:03:39.099664Z","shell.execute_reply.started":"2024-06-20T11:03:39.095183Z","shell.execute_reply":"2024-06-20T11:03:39.098695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The code import warnings;warnings.fiterwarnings(\"ignore\") imports the warnings module and sets the warning filter to ignore. This means all warnings will be suppressed and will not be displayed. Actually during training there are many warnings that do not prevent the fine-tuning but can be distracting and make you wonder if you are doing the correct things.","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\nprint(\"1\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:03:41.048131Z","iopub.execute_input":"2024-06-20T11:03:41.048555Z","iopub.status.idle":"2024-06-20T11:03:41.054250Z","shell.execute_reply.started":"2024-06-20T11:03:41.048524Z","shell.execute_reply":"2024-06-20T11:03:41.053212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U \"accelerate==0.26.1\" ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U \"bitsandbytes==0.42.0\"","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:43:57.029549Z","iopub.execute_input":"2024-06-20T10:43:57.030208Z","iopub.status.idle":"2024-06-20T10:44:14.654758Z","shell.execute_reply.started":"2024-06-20T10:43:57.030171Z","shell.execute_reply":"2024-06-20T10:44:14.653510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U  \"transformers==4.38.2\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q -U  \"datasets==2.16.1\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow[and-cuda]","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:49:14.655167Z","iopub.execute_input":"2024-06-20T10:49:14.656042Z","iopub.status.idle":"2024-06-20T10:53:14.941058Z","shell.execute_reply.started":"2024-06-20T10:49:14.656010Z","shell.execute_reply":"2024-06-20T10:53:14.940037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install -q -U git+https://github.com/huggingface/peft@4a1559582281fc3c9283892caea8ccef1d6f5a4f","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:44:35.884863Z","iopub.execute_input":"2024-06-20T10:44:35.885251Z","iopub.status.idle":"2024-06-20T10:45:05.128743Z","shell.execute_reply.started":"2024-06-20T10:44:35.885209Z","shell.execute_reply":"2024-06-20T10:45:05.127494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip uninstall keras\n!pip install tensorflow","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:00:42.193551Z","iopub.execute_input":"2024-06-20T11:00:42.194343Z","iopub.status.idle":"2024-06-20T11:03:03.354004Z","shell.execute_reply.started":"2024-06-20T11:00:42.194308Z","shell.execute_reply":"2024-06-20T11:03:03.352666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/trl.git@7630f877f91c556d9e5a3baa4b6e2894d90ff84c","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:45:14.221663Z","iopub.execute_input":"2024-06-20T10:45:14.221993Z","iopub.status.idle":"2024-06-20T10:45:43.166508Z","shell.execute_reply.started":"2024-06-20T10:45:14.221970Z","shell.execute_reply":"2024-06-20T10:45:43.165283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:03:20.066312Z","iopub.execute_input":"2024-06-20T11:03:20.066861Z","iopub.status.idle":"2024-06-20T11:03:20.074799Z","shell.execute_reply.started":"2024-06-20T11:03:20.066721Z","shell.execute_reply":"2024-06-20T11:03:20.073757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"pytorch version {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:03:50.370443Z","iopub.execute_input":"2024-06-20T11:03:50.371099Z","iopub.status.idle":"2024-06-20T11:03:50.375679Z","shell.execute_reply.started":"2024-06-20T11:03:50.371070Z","shell.execute_reply":"2024-06-20T11:03:50.374731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"working on {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:53:37.029468Z","iopub.execute_input":"2024-06-20T10:53:37.030537Z","iopub.status.idle":"2024-06-20T10:53:37.037571Z","shell.execute_reply.started":"2024-06-20T10:53:37.030503Z","shell.execute_reply":"2024-06-20T10:53:37.035942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing the data and the core evaluation functioins\nThe code in the next cell performs the following steps:\n1. Reads the input dataset from the all-data.csv file, which is a comma-separated value(CSV) file with two columns: sentiment and text.\n2. Splits the dataset into training and test sets,with 300 samples in each set. The split is stratified by sentiment, so that each set contains a representative of positive,neutral, and negative sentiments.\n3. Shuffles the train data in a replicable order(random_state=10)\n4. Transfoms the texts contained in the train and test data into prompts to be used by LLamMa: the train prompts contains the expected answer we want to fine-tune the model-with\n5. The residual examples not in train or test, for reporting purposes during during training (but it won't be used for early stopping), is treated as evaluatio  data, which is sampled with repetition in order to have a 50/50/50 sample (negative instances are very few, hence the shoud be repeated)\n6. The train and eval data are wrapped by the class from HuggingFace's datasets library(backed by the Apache Arrow format)\n\nThis prepares in a single cell train_data, eval_data and test_data datasets to be used in the fine tuning.","metadata":{}},{"cell_type":"code","source":"filename = \"../input/sentiment-analysis-for-financial-news/all-data.csv\"\n\ndf = pd.read_csv(filename, \n                 names=[\"sentiment\", \"text\"],\n                 encoding=\"utf-8\", encoding_errors=\"replace\")\n\nX_train = list()\nX_test = list()\nfor sentiment in [\"positive\", \"neutral\", \"negative\"]:\n    train, test  = train_test_split(df[df.sentiment==sentiment], \n                                    train_size=300,\n                                    test_size=300, \n                                    random_state=42)\n    X_train.append(train)\n    X_test.append(test)\n\nX_train = pd.concat(X_train).sample(frac=1, random_state=10)\nX_test = pd.concat(X_test)\n\neval_idx = [idx for idx in df.index if idx not in list(X_train.index) + list(X_test.index)]\nX_eval = df[df.index.isin(eval_idx)]\nX_eval = (X_eval\n          .groupby('sentiment', group_keys=False)\n          .apply(lambda x: x.sample(n=50, random_state=10, replace=True)))\nX_train = X_train.reset_index(drop=True)\n\ndef generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = {data_point[\"sentiment\"]}\n            \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the sentiment of the news headline enclosed in square brackets, \n            determine if it is positive, neutral, or negative, and return the answer as \n            the corresponding sentiment label \"positive\" or \"neutral\" or \"negative\".\n\n            [{data_point[\"text\"]}] = \"\"\".strip()\n\nX_train = pd.DataFrame(X_train.apply(generate_prompt, axis=1), \n                       columns=[\"text\"])\nX_eval = pd.DataFrame(X_eval.apply(generate_prompt, axis=1), \n                      columns=[\"text\"])\n\ny_true = X_test.sentiment\nX_test = pd.DataFrame(X_test.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T08:41:30.470655Z","iopub.execute_input":"2024-06-21T08:41:30.474257Z","iopub.status.idle":"2024-06-21T08:41:30.964268Z","shell.execute_reply.started":"2024-06-21T08:41:30.474146Z","shell.execute_reply":"2024-06-21T08:41:30.961598Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/sentiment-analysis-for-financial-news/all-data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(filename, \n\u001b[1;32m      4\u001b[0m                  names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m                  encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding_errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[1;32m      8\u001b[0m X_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n","\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"],"ename":"NameError","evalue":"name 'pd' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"Next part to do is creating a function to evaluate the results from the fine-tuned sentiment model. The function performs the following setps\"\n1. Maps the sentiment labels to a numeriacal representation, where 2 represents positive, 1 represents neutral, and 0 represents negative.\n2. Calculates the accuracy of the model on the test data.\n3. Generates an accuracy report for each sentiment labal.\n4. Generates a classification report for the model.\n5. Generates a confusion matrix for the model.","metadata":{}},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['positive', 'neutral', 'negative']\n    mapping = {'positive': 2, 'neutral': 1, 'none':1, 'negative': 0}\n    def map_func(x):\n        return mapping.get(x, 1)\n    \n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n    \n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n    \n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true)) \n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n        \n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n    \n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T10:47:08.096890Z","iopub.execute_input":"2024-06-20T10:47:08.097259Z","iopub.status.idle":"2024-06-20T10:47:08.106881Z","shell.execute_reply.started":"2024-06-20T10:47:08.097230Z","shell.execute_reply":"2024-06-20T10:47:08.106032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the model without fine-tuning\n\nNext we need to take care of the model, which is a 7b-hf(7 billion parameters, no RLHF(Reinforcement Learning From Human Feedback), in the HuggingFace compatible format), loading from Kaggle models and quantization.\n\nModel loading and quantization:\n- First the code loads the LLaMA2 ","metadata":{}},{"cell_type":"markdown","source":"#### docs of BitsAndByteConfig(https://huggingface.co/docs/transformers/main/en/main_classes/quantization#transformers.BitsAndBytesConfig)\n- load_in_4bit (bool, optional, defaults to False) — This flag is used to enable 4-bit quantization by replacing the Linear layers with FP4(4-bit floating-point)/NF4((normalized float 4) layers from bitsandbytes.\n- bnb_4bit_quant_type (str, optional, defaults to \"fp4\") — This sets the quantization data type in the bnb.nn.Linear4Bit layers. Options are FP4 and NF4 data types which are specified by fp4 or nf4.\n- bnb_4bit_compute_dtype (torch.dtype or str, optional, defaults to torch.float32) — This sets the computational type which might be different than the input type. For example, inputs might be fp32, but computation can be set to bf16 for speedups.\n-bnb_4bit_use_double_quant (bool, optional, defaults to False) — This flag is used for nested quantization where the quantization constants from the first quantization are quantized again.","metadata":{}},{"cell_type":"code","source":"model_name = \"../input/llama-2/pytorch/7b-hf/1\"\n\ncompute_dtype = getattr(torch, \"float16\")\n\n\nbnb_config = BitsAndBytesConfig(\n    \n    load_in_4bit=True, \n    bnb_4bit_quant_type=\"nf4\", \n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=device,\n    torch_dtype=compute_dtype,\n    quantization_config=bnb_config, \n)\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, \n                                          trust_remote_code=True,\n                                         )\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel, tokenizer = setup_chat_format(model, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-20T11:04:07.819226Z","iopub.execute_input":"2024-06-20T11:04:07.819611Z","iopub.status.idle":"2024-06-20T11:05:35.562719Z","shell.execute_reply.started":"2024-06-20T11:04:07.819582Z","shell.execute_reply":"2024-06-20T11:05:35.561940Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}